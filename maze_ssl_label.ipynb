{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from torchvision.transforms import Resize\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "# load filenames\n",
    "with open('filenames.pickle', 'rb') as f:\n",
    "    filenames = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for preprocessing images\n",
    "\n",
    "def _preprocess_images(images):\n",
    "    assert images.dtype == np.uint8, 'image need to be uint8!'\n",
    "    images = resize_video(images, (32, 32))\n",
    "    images = np.transpose(images, [0, 3, 1, 2])  # convert to channel-first\n",
    "    images = images.astype(np.float32) / 255 * 2 - 1\n",
    "    assert images.dtype == np.float32, 'image need to be float32!'\n",
    "    return images\n",
    "\n",
    "def resize_video(video, size):\n",
    "    if video.shape[1] == 3:\n",
    "        video = np.transpose(video, (0,2,3,1))\n",
    "    transformed_video = np.stack([np.asarray(Resize(size)(Image.fromarray(im))) for im in video], axis=0)\n",
    "    return transformed_video\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        # Take care that getattr() raises AttributeError, not KeyError.\n",
    "        # Required e.g. for hasattr(), deepcopy and OrderedDict.\n",
    "        try:\n",
    "            return self.__getitem__(attr)\n",
    "        except KeyError:\n",
    "            raise AttributeError(\"Attribute %r not found\" % attr)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self = d\n",
    "        \n",
    "\n",
    "def sample_batch_function() :\n",
    "    # sample 128 state-action pairs from 79131 trajectories\n",
    "    one_batch_indexes = list(np.random.randint(0, len(filenames)-1, size=128))\n",
    "\n",
    "    prep_data = AttrDict()\n",
    "\n",
    "    st_i = 0\n",
    "\n",
    "    for i in one_batch_indexes :\n",
    "\n",
    "        st_i += 1\n",
    "        index = i\n",
    "        data = AttrDict()\n",
    "        samples_per_file = 1\n",
    "        file_index = index // samples_per_file\n",
    "        path = filenames[file_index]\n",
    "\n",
    "\n",
    "        try:\n",
    "            with h5py.File(path, 'r') as F:\n",
    "                ex_index = index % samples_per_file  # get the index\n",
    "                key = 'traj{}'.format(ex_index)\n",
    "\n",
    "                # Fetch data into a dict\n",
    "                for name in F[key].keys():\n",
    "                    if name in ['states', 'actions', 'pad_mask']:\n",
    "                        data[name] = F[key + '/' + name][()].astype(np.float32)\n",
    "\n",
    "                if key + '/images' in F:\n",
    "                    data.images = F[key + '/images'][()]\n",
    "                else:\n",
    "                    data.images = np.zeros((data.states.shape[0], 2, 2, 3), dtype=np.uint8)\n",
    "                    \n",
    "                # terminals\n",
    "                data.terminals = np.full((data.states.shape[0],), False, dtype=bool)\n",
    "                data.terminals[-1] = True\n",
    "                \n",
    "        except:\n",
    "            raise ValueError(\"Could not load from file {}\".format(path))\n",
    "\n",
    "\n",
    "        data.images = _preprocess_images(data.images)\n",
    "        one_transition = np.random.randint(1, data.states.shape[0]-2, size=1)\n",
    "        data.conc_images = np.concatenate((data.images[one_transition-1], data.images[one_transition]),axis=1)\n",
    "        data.next_conc_images = np.concatenate((data.images[one_transition], data.images[one_transition+1]),axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        if st_i == 1 :\n",
    "\n",
    "            prep_data.actions = data.actions[one_transition]\n",
    "            prep_data.images = data.images[one_transition]\n",
    "            prep_data.conc_images = data.conc_images \n",
    "            prep_data.next_conc_images = data.next_conc_images\n",
    "        \n",
    "        else :\n",
    "            prep_data.actions = np.concatenate((prep_data.actions, data.actions[one_transition]),axis=0)\n",
    "            prep_data.images = np.concatenate((prep_data.images, data.images[one_transition]),axis=0)\n",
    "            prep_data.conc_images = np.concatenate((prep_data.conc_images, data.conc_images),axis=0)\n",
    "            prep_data.next_conc_images = np.concatenate((prep_data.next_conc_images, data.next_conc_images),axis=0)\n",
    "    \n",
    "    return prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised NN for extracting features from multi-goal datasets\n",
    "\n",
    "class Phi(nn.Module): #A\n",
    "    def __init__(self):\n",
    "        super(Phi, self).__init__()\n",
    "        # state : (32, 32, 3)\n",
    "        # state : (128, 60) -> phi(st) : (128, 288)\n",
    "        self.conv1 = nn.Conv2d(6, 8, kernel_size=(4,4), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(4,4), stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(4,4), stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(4,4), stride=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # torch.Size([128, 2, 3, 32, 32])\n",
    "        #x = F.normalize(x)\n",
    "\n",
    "        # torch.Size([128, 32, 2, 16, 16])\n",
    "        y = F.leaky_relu(self.conv1(x), negative_slope=0.2, inplace=True)\n",
    "        y = F.leaky_relu(self.bn2(self.conv2(y)), negative_slope=0.2, inplace=True)\n",
    "        y = F.leaky_relu(self.bn3(self.conv3(y)), negative_slope=0.2, inplace=True)\n",
    "        y = self.conv4(y)\n",
    "        \n",
    "        # torch.Size([128, 34])\n",
    "        y = y.flatten(start_dim=1) \n",
    "        return y\n",
    "\n",
    "class Gnet(nn.Module): #B\n",
    "    def __init__(self):\n",
    "        super(Gnet, self).__init__()\n",
    "        \n",
    "        #\n",
    "        self.linear1 = nn.Linear(64,32)\n",
    "        self.linear2 = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, state1,state2):\n",
    "        \n",
    "        # phi(st) : (128, 32) + phi(st+1) : (128, 32) = (128, 64)\n",
    "        x = torch.cat( (state1, state2) ,dim=1)\n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = self.linear2(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class Fnet(nn.Module): #C\n",
    "    def __init__(self):\n",
    "        super(Fnet, self).__init__()\n",
    "        \n",
    "        # phi(st) : (128, 32) + action : (128, 2) = (128, 34)\n",
    "        # (128, 297) -> (128, 1024)\n",
    "        self.linear1 = nn.Linear(34,52)\n",
    "        self.linear2 = nn.Linear(52,32)\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        x = torch.cat((state,action) ,dim=1)\n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = self.linear2(y)\n",
    "        return y\n",
    "    \n",
    "params = {\n",
    "    'batch_size':150,\n",
    "    'beta':0.2,\n",
    "    'lambda':0.1,\n",
    "    'eta': 1.0,\n",
    "    'gamma':0.2,\n",
    "    'max_episode_len':100,\n",
    "    'min_progress':15,\n",
    "    'action_repeats':6,\n",
    "    'frames_per_state':3\n",
    "}\n",
    "\n",
    "\n",
    "def loss_fn(inverse_loss, forward_loss):\n",
    "    # loss_  = torch.Size([128, 1])\n",
    "    loss_ = (1 - params['beta']) * inverse_loss\n",
    "    loss_ += params['beta'] * forward_loss\n",
    "    \n",
    "    # loss_.flatten() : torch.Size([128])\n",
    "    loss = loss_.sum() / loss_.flatten().shape[0]\n",
    "    return loss\n",
    "\n",
    "\n",
    "def ICM(state1, action, state2, forward_scale=1., inverse_scale=1e4):\n",
    "    state1_hat = encoder(state1) #A\n",
    "    state2_hat = encoder(state2)\n",
    "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach()) #B\n",
    "    \n",
    "    # forward_loss : torch.Size([128, 288])\n",
    "    # forward_loss.sum(dim=1) : torch.Size([128])\n",
    "    # forward_loss.sum(dim=1).unsqueeze(dim=1) : torch.Size([128, 1])\n",
    "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, \\\n",
    "                        state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    \n",
    "    # torch.Size([128, 9])\n",
    "    pred_action = inverse_model(state1_hat, state2_hat) #C\n",
    "    \n",
    "    # inverse_loss : torch.Size([128, 9])\n",
    "    # inverse_loss.sum(dim=1) : torch.Size([128])\n",
    "    # inverse_loss.sum(dim=1).unsqueeze(dim=1) : torch.Size([128, 1])\n",
    "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, \\\n",
    "                                        action.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    return forward_pred_err, inverse_pred_err\n",
    "\n",
    "\n",
    "def minibatch_train(use_extrinsic=True):\n",
    "\n",
    "    batch_loaded = sample_batch_function()\n",
    "\n",
    "    state1_batch = torch.from_numpy(batch_loaded.conc_images).to(\"cuda\")\n",
    "    state2_batch = torch.from_numpy(batch_loaded.next_conc_images).to(\"cuda\")\n",
    "    action_batch = torch.from_numpy(batch_loaded.actions).to(\"cuda\")\n",
    "\n",
    "    \n",
    "    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch) #B\n",
    "    i_reward = (1. / params['eta']) * forward_pred_err #C\n",
    "    i_reward = i_reward.detach() #D\n",
    " \n",
    "    return forward_pred_err, inverse_pred_err, i_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Phi().to(\"cuda\")\n",
    "forward_model = Fnet().to(\"cuda\")\n",
    "inverse_model = Gnet().to(\"cuda\")\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "inverse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "all_model_params = list(encoder.parameters()) #A\n",
    "all_model_params += list(forward_model.parameters()) + list(inverse_model.parameters())\n",
    "opt = optim.Adam(lr=0.001, params=all_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:07<12:22,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2354.513184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:46<05:44,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1863.161255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:48<06:35,  4.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/app/spirl/maze_baseline.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     forward_pred_err, inverse_pred_err, i_reward \u001b[39m=\u001b[39m minibatch_train(use_extrinsic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m#H\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(forward_pred_err, inverse_pred_err) \u001b[39m#I\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     loss_list \u001b[39m=\u001b[39m (forward_pred_err\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mmean(), inverse_pred_err\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mmean())\n",
      "\u001b[1;32m/app/spirl/maze_baseline.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminibatch_train\u001b[39m(use_extrinsic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     batch_loaded \u001b[39m=\u001b[39m sample_batch_function()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     state1_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(batch_loaded\u001b[39m.\u001b[39mconc_images)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     state2_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(batch_loaded\u001b[39m.\u001b[39mnext_conc_images)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/app/spirl/maze_baseline.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not load from file \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m data\u001b[39m.\u001b[39mimages \u001b[39m=\u001b[39m _preprocess_images(data\u001b[39m.\u001b[39;49mimages)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# last state is terminate state, so remove it\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# we need to have previous state, since we stack two consecutive 32 × 32px observations \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# as input to the policy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# so the current state is (2, 32, 32, 3), where it is the stack of (last_image, current image)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# input of shape is [batch, channel, height, width]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m one_transition \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m1\u001b[39m, data\u001b[39m.\u001b[39mstates\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/app/spirl/maze_baseline.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_preprocess_images\u001b[39m(images):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39massert\u001b[39;00m images\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39muint8, \u001b[39m'\u001b[39m\u001b[39mimage need to be uint8!\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     images \u001b[39m=\u001b[39m resize_video(images, (\u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(images, [\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m])  \u001b[39m# convert to channel-first\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/app/spirl/maze_baseline.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m video\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(video, (\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m transformed_video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([np\u001b[39m.\u001b[39masarray(Resize(size)(Image\u001b[39m.\u001b[39mfromarray(im))) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m video], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m transformed_video\n",
      "\u001b[1;32m/app/spirl/maze_baseline.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m video\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(video, (\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m transformed_video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([np\u001b[39m.\u001b[39;49masarray(Resize(size)(Image\u001b[39m.\u001b[39;49mfromarray(im))) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m video], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f537069524c5f6d617a655f7261735f7072696f7232222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3136312e3536227d7d/app/spirl/maze_baseline.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m transformed_video\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:513\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaccess \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exif \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    515\u001b[0m         deprecate(\u001b[39m\"\u001b[39m\u001b[39mImage categories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_animated\u001b[39m\u001b[39m\"\u001b[39m, plural\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train unsupervised NN for extracting features from multi-goal datasets\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "eps=0.15\n",
    "losses = []\n",
    "episode_length = 0\n",
    "\n",
    "ep_lengths = []\n",
    "use_explicit = False\n",
    "\n",
    "i_reward_list = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    forward_pred_err, inverse_pred_err, i_reward = minibatch_train(use_extrinsic=False) #H\n",
    "    loss = loss_fn(forward_pred_err, inverse_pred_err) #I\n",
    "    loss_list = (forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean())\n",
    "    losses.append(loss_list)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print\n",
    "    if i % 10 == 0:\n",
    "        loss = loss.item()\n",
    "        print(f\"loss: {loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
